{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8-MVMbXWVks"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import torch\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random\n",
        "import datetime\n",
        "import numpy as np\n",
        "from skimage import transform\n",
        "\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
        "\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "# Create Super Mario Bros environment\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "\n",
        "# Define action space\n",
        "action_space = SIMPLE_MOVEMENT\n",
        "\n",
        "# Wrap the environment with JoypadSpace\n",
        "env = JoypadSpace(env, action_space)\n",
        "\n",
        "# Define observation preprocessing wrappers\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        resize_obs = transform.resize(observation, self.shape)\n",
        "        # cast float back to uint8\n",
        "        resize_obs *= 255\n",
        "        resize_obs = resize_obs.astype(np.uint8)\n",
        "        return resize_obs\n",
        "\n",
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "# Apply observation preprocessing\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env, keep_dim=False)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "env = TransformObservation(env, f=lambda x: x / 255.)\n",
        "env = FrameStack(env, num_stack=4)\n",
        "\n",
        "# Define Double Deep Q-Network (DDQN)\n",
        "class DDQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        c, h, w = input_dim\n",
        "\n",
        "        if h != 84 or w != 84:\n",
        "            raise ValueError(\"Expecting input height and width to be 84.\")\n",
        "\n",
        "        self.online = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "\n",
        "        self.target = copy.deepcopy(self.online)\n",
        "\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        if model == 'online':\n",
        "            return self.online(input)\n",
        "        elif model == 'target':\n",
        "            return self.target(input)\n",
        "\n",
        "# Define DDQN Agent\n",
        "class DDQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "\n",
        "        self.net = DDQN(self.state_dim, self.action_dim).float()\n",
        "        if self.use_cuda:\n",
        "            self.net = self.net.to(device='cuda')\n",
        "\n",
        "        self.exploration_rate = 1\n",
        "        self.exploration_rate_decay = 0.99999975\n",
        "        self.exploration_rate_min = 0.1\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.9\n",
        "        self.burnin = 1e5\n",
        "        self.learn_every = 3\n",
        "        self.sync_every = 1e4\n",
        "        self.save_every = 5e5\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "        else:\n",
        "            state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
        "            state = state.unsqueeze(0)\n",
        "            action_values = self.net(state, model='online')\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # Ensure action index is within valid range\n",
        "        action_idx = min(action_idx, self.action_dim - 1)\n",
        "\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "        self.curr_step += 1\n",
        "        return action_idx\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
        "        next_state = torch.FloatTensor(next_state).cuda() if self.use_cuda else torch.FloatTensor(next_state)\n",
        "        action = torch.LongTensor([action]).cuda() if self.use_cuda else torch.LongTensor([action])\n",
        "        reward = torch.DoubleTensor([reward]).cuda() if self.use_cuda else torch.DoubleTensor([reward])\n",
        "        done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n",
        "\n",
        "        self.memory.append((state, next_state, action, reward, done))\n",
        "\n",
        "    def learn(self):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        if self.curr_step % self.save_every == 0:\n",
        "            self.save()\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "\n",
        "        # Sample from memory\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # Get TD Estimate\n",
        "        td_est = self.td_estimate(state, action)\n",
        "\n",
        "        # Get TD Target\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # Backpropagate loss through Q_online\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        current_Q = self.net(state, model='online')[np.arange(0, self.batch_size), action]\n",
        "        return current_Q\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        next_state_Q = self.net(next_state, model='online')\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model='target')[np.arange(0, self.batch_size), best_action]\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
        "\n",
        "    def recall(self):\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
        "\n",
        "    def save(self):\n",
        "        save_path = self.save_dir / f\"ddqn_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "        torch.save(\n",
        "            dict(\n",
        "                model=self.net.state_dict(),\n",
        "                exploration_rate=self.exploration_rate\n",
        "            ),\n",
        "            save_path\n",
        "        )\n",
        "        print(f\"DDQN saved to {save_path} at step {self.curr_step}\")\n",
        "\n",
        "# Main function\n",
        "from pathlib import Path\n",
        "\n",
        "def main():\n",
        "    # Define save directory\n",
        "    save_dir = Path('checkpoints') / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Create the environment\n",
        "    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "    env = JoypadSpace(\n",
        "        env,\n",
        "        [['right'],\n",
        "         ['right', 'A']]\n",
        "    )\n",
        "\n",
        "    # Creating the DDQN agent\n",
        "    action_dim = 7  # Manually specify the number of actions\n",
        "    ddqn_agent = DDQNAgent(state_dim=(4, 84, 84), action_dim=action_dim, save_dir=save_dir)\n",
        "\n",
        "    episodes = 10000\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state= env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # Render the environment\n",
        "            env.render()\n",
        "\n",
        "            action = ddqn_agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            ddqn_agent.cache(state, next_state, action, reward, done)\n",
        "\n",
        "            q, loss = ddqn_agent.learn()\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done or total_reward > 300:  # Arbitrary break condition\n",
        "                print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
        "                break\n",
        "\n",
        "    env.close()  # Close the environment after training\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ]
}